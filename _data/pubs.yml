papers:
  
  - authors:
    - S. Bordt
    - S. Srinivas
    - V. Boreiko
    lastauthor: U. Luxburg
    year: 2024
    journal: 
    title: How much can we forget about Data Contamination?
    tldr: Are LLM benchmarks rendered invalid for any amount of test set contamination in the pre-training data? It turns out not always, because models also naturally forget examples seen during training.
    url: https://arxiv.org/abs/2410.03249
    code: 
    type: preprint


  - authors:
    - U. Bhalla
    - S. Srinivas
    - A. Ghandeharioun
    lastauthor: H. Lakkaraju
    year: 2024
    journal: 
    title: Towards Unifying Interpretability and Control&colon; Evaluation via Intervention
    tldr: Popular mech interp methods such as sparse autoencoders underperform simpler alternatives such as prompting and logitlens on the task of controlling model outputs, raising questions on the faithfulness of such methods.
    url: https://arxiv.org/abs/2411.04430
    code: 
    type: preprint

  
  - authors:
    - D. Ley
    - S. Srinivas
    - S. Zhang
    - G. Rusak
    lastauthor: H. Lakkaraju
    year: 2024
    journal: 
    title: Generalized Group Data Attribution
    tldr: Data attribution methods, such as influence functions, can be made drastically more efficient (10-50x) by attributing to groups rather than individual data points. This can be used for fast dataset pruning and noisy label identification. 
    url: https://arxiv.org/abs/2410.09940
    code: 
    type: preprint

  - authors:
    - C. Badrinath
    - U. Bhalla
    - A. Oesterling
    - S. Srinivas
    lastauthor: H. Lakkaraju
    year: 2024 
    journal: ICML Workshops
    title: All Roads Lead to Rome? Exploring Representational Similarities Between Latent Spaces of Generative Image Models
    url: https://arxiv.org/abs/2407.13449
    code:
    tldr: We find that many generative image models recover approximately similar representations.
    type: short
    notes: Published at the Workshop on Geometry-grounded Representation Learning and Generative Modeling (GRaM)
 
  - authors: 
    - U. Bhalla*
    - A. Oesterling*
    - S. Srinivas
    - F. Calmon
    lastauthor: H. Lakkaraju
    year: 2024
    journal: NeurIPS
    title: Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)
    url: https://arxiv.org/abs/2402.10376
    code: https://github.com/AI4LIFE-GROUP/SpLiCE
    tldr: We convert dense uninterpretable CLIP embeddings to overcomplete sparse interpretable ones, with a minimal loss in fidelity. 
    type: long
    featured: true

 
  - authors: 
    - A. Kumar
    - C. Agarwal
    - S. Srinivas
    - A. Li
    - S. Feizi
    lastauthor: H. Lakkaraju
    year: 2024
    journal: CoLM
    title: Certifying LLM Safety against Adversarial Prompting
    url: https://arxiv.org/abs/2309.02705
    code: https://github.com/aounon/certified-llm-safety
    tldr: We present a simple method to detect LLM adversarial attacks, by systematically deleting tokens until the underlying string is labelled harmful.
    type: long

  - authors: 
    - T. Han*
    - S. Srinivas*
    lastauthor: H. Lakkaraju
    year: 2024 
    journal: UAI
    title: Characterizing Data Point Vulnerability as Average-Case Robustness
    url: https://arxiv.org/abs/2307.13885
    tldr: We consider a relaxation of adversarial robustness, i.e., average-case robustness, and provide efficient estimators to compute this quantity. 
    type: long

  - authors:
    - S. Srinivas*
    - S. Bordt*
    lastauthor: H. Lakkaraju
    year: 2023
    journal: NeurIPS
    title: Which Models have Perceptually-Aligned Gradients? An Explanation via Off-Manifold Robustness
    tldr: Previous work finds gradients of robust models to be "perceptually aligned". We explain this phenomenon by observing that robust models in practice are not robust in all directions, in fact they are mostly only robust outside the data manifold. This causes their gradients to align with the manifold, causing them to be perceptually aligned. 
    url: https://arxiv.org/abs/2305.19101
    code: https://github.com/tml-tuebingen/pags
    type: long
    featured: true
    notes: <font color="crimson">Spotlight presentation (Top 3%)</font>

  - authors:
    - U. Bhalla*
    - S. Srinivas*
    lastauthor: H. Lakkaraju
    year: 2023
    journal: NeurIPS
    title: Discriminative Feature Attributions&colon; A Bridge between Post Hoc Explainability and Inherent Interpretability
    tldr: Given a pre-trained model, adapt this model to be robust to the perturbations introduced by feature attribution methods. Doing so results in models that recover ground truth attributions!
    url: https://arxiv.org/abs/2307.15007
    code: https://github.com/AI4LIFE-GROUP/DiET
    type: long

  - authors: 
    - A. Lin
    - L.M. Paes
    - S.H. Tanneru
    - S. Srinivas
    lastauthor: H. Lakkaraju
    year: 2023
    journal: ICML Workshops
    title: Word-Level Explanations for Analyzing Bias in Text-to-Image Models
    url: https://arxiv.org/abs/2306.05500
    tldr: For text to image models, we find which input words contribute to bias in output images. For example, we find that the word "doctor" in the input leads to an over-representation of males in the output.
    type: short
    notes: Published at the Workshop on Challenges in Deploying Generative AI

  - authors:
    - D. Ley
    - L. Tang
    - M. Nazari
    - H. Lin
    - S. Srinivas
    lastauthor: H. Lakkaraju
    year: 2023
    journal: ICML Workshops
    title: Consistent Explanations in the Face of Model Indeterminacy via Ensembling
    url: https://arxiv.org/abs/2306.06193
    tldr: With model ensembles, feature attributions are fairly consistent. We find strategies that lead to efficient construction of such ensembles.
    type: short 
    notes: Published at the Workshop on Interpretable Machine Learning for Healthcare (IMLH)

  - authors:
    - A. Meyer*
    - D. Ley*
    - S. Srinivas
    lastauthor: H. Lakkaraju
    year: 2023
    journal: UAI 
    title: On Minimizing the Impact of Dataset Shifts on Actionable Explanations
    tldr: How to train classifiers such that they are unaffected by small shifts in the dataset? We show theoretically and experimentally that weight decay, model curvature and robustness are all important factors that can help minimize the impact of such dataset shifts.
    url: https://arxiv.org/abs/2306.06716
    notes: <font color="crimson">Oral presentation (Top 5%)</font>
    type: long

  - authors:
    - T. Han
    - S. Srinivas
    lastauthor: H. Lakkaraju
    year: 2022
    journal: NeurIPS
    title: Which Explanation Should I Choose? A Function Approximation Perspective to Characterizing Post hoc Explanations
    tldr: Several popular post-hoc explanations such as LIME, SHAP, and gradient based explanations can be viewed as performing local function approximation (LFA). Thinking of LFA as a framework for explanations enables us to make useful statements about explanations such as a no-free lunch theorem, and identify which explanations to use.
    url: https://arxiv.org/abs/2206.01254
    notes: <font color="crimson">Best paper award</font> at ICML "Interpretable ML for Healthcare" workshop, 2022
    workshop: https://icml.cc/virtual/2022/workshop/13449
    code: https://github.com/AI4LIFE-GROUP/lfa
    type: long
    featured: true

  - authors:
      - S. Srinivas*
      - K. Matoba*
      - H. Lakkaraju
    lastauthor: F. Fleuret
    year: 2022
    journal: NeurIPS
    title: Efficiently Training Low-Curvature Neural Networks
    tldr: We train low-curvature neural networks, that are "as linear as possible" by (1) replacing ReLU with a variant of softplus, (2) spectral normalization of linear layers, (3) (optionally) using gradient-norm regularization; and minimizing the curvatures and spectral norms of each layer independently. This approach rivals adversarial training without training with adversarial examples.
    url: https://arxiv.org/abs/2206.07144
    code: https://github.com/kylematoba/lcnn
    poster: pdfs/lcnn_poster.pdf
    presentation: https://nips.cc/virtual/2022/poster/54125
    type: long
    featured: true

  - authors:
    - M. El-Halabi
    - S. Srinivas
    lastauthor: S. Lacoste-Julien
    year: 2022
    journal: NeurIPS
    title: Data-Efficient Structured Pruning via Submodular Optimization
    tldr: Pruning neurons in neural networks can be cast as a submodular optimization problem, enabling proposal of principled algorithms with rigorous theoretical guarantees that perform well when pruning with small number of data points.  
    url: https://arxiv.org/abs/2203.04940
    type: long

  - authors:
    - S. Srinivas
    - A. Kuzmin
    - M. Nagel
    - M. van Baalen
    - A. Skliar
    lastauthor: T. Blankevoort
    year: 2022
    journal: CVPR Workshops
    title: Cyclical Pruning for Sparse Neural Networks
    tldr: Algorithms for training sparse neural networks should be more like projected gradient descent / iterative hard thresholding, which alternates between sparsification (i.e., projection step) and densification (i.e., gradient step), as opposed to common pruning approaches which do not perform densification.
    url: https://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Srinivas_Cyclical_Pruning_for_Sparse_Neural_Networks_CVPRW_2022_paper.html
    presentation: https://docs.google.com/presentation/d/1lU1aZGrznxakE09H5MtmqXndR66hmZhB9ULwM9uVO4c/edit?usp=sharing
    notes: <font color="crimson">Oral presentation</font> at the Workshop on Efficient Computer Vision for Deep Learning (ECV) 
    type: short
  
  - authors:
      - S. Srinivas
    lastauthor: F. Fleuret
    year: 2021
    journal: ICLR
    title: Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability
    tldr: Commonly used input-gradient saliency maps for explaining discriminative neural nets capture information about an implicit density model, rather than that of the underlying discriminative model which it is intended to explain.
    url: https://openreview.net/forum?id=dYeAHXnpWJ4
    presentation: https://docs.google.com/presentation/d/1v35LGiLpFDSN2okOC9qxWjQ8MiHkYAMeMUCU8U3dFzE/edit?usp=sharing
    code: https://github.com/idiap/rethinking-saliency/
    workshop: https://sites.google.com/view/whi2020/
    notes: <font color="crimson">Oral presentation (Top 1%)</font> 
    poster: pdfs/iclr2021_poster.pdf
    type: long

  - authors:
      - S. Srinivas
    lastauthor: F. Fleuret
    year: 2019
    journal: NeurIPS
    title: Full-Gradient Representation for Neural Network Visualization
    url: https://papers.nips.cc/paper/2019/hash/80537a945c7aaa788ccfcdf1b99b5d8f-Abstract.html
    code: https://github.com/idiap/fullgrad-saliency
    tldr: Compute saliency information from all intermediate layers in neural networks, rather than just from the input, as is done commonly. This provably captures two desirable properties (sensitivity and completeness) which typical saliency maps cannot capture.
    poster: pdfs/neurips19_poster.pdf 
    type: long
    notes: 190+ Github stars &centerdot; 200+ citations

  - authors:
      - S. Srinivas
    lastauthor: F. Fleuret
    year: 2018
    journal: ICML
    title: Knowledge Transfer with Jacobian Matching
    url: http://proceedings.mlr.press/v80/srinivas18a.html
    presentation: pdfs/icml18_presentation.pdf
    poster: pdfs/icml18_poster.pdf
    workshop: https://lld-workshop.github.io/2017/
    notes: <font color="crimson">Best paper award</font> at NeurIPS "Learning with Limited Data" workshop, 2017
    tldr: Perform sample-efficient distillation by requiring that the student model mimic the input-gradients of the teacher model. This is equivalent (in expectation) to performing classical distillation with data augmentation via additive input noise.
    type: long
    featured: true

  - authors:
      - A. Subramanya
      - S. Srinivas
    lastauthor: R.V. Babu
    year: 2018
    journal: SPCOM
    title: Estimating Confidence for Deep Neural Networks through Density modelling
    url: https://ieeexplore.ieee.org/abstract/document/8724461/
    presentation: https://docs.google.com/presentation/d/1YBKlngRwfDgWpy5mEIj1zAP2dx8IAWlG7ejsPCBn_E0/edit?usp=sharinghttp://proceedings.mlr.press/v80/srinivas18a.html
    tldr: Model the density of intermediate features in a neural network using a high-dimensional Gaussian distribution. If features for a test point fall outside the "typical set" for such a Gaussian, then declare that test point to be out-of-distribution.
    type: short

  - authors:
      - S. Srinivas
      - A. Subramanya
    lastauthor: R.V. Babu
    year: 2017
    journal: CVPR Workshops
    title: Training Sparse Neural Networks
    url: https://arxiv.org/abs/1611.06694
    presentation: https://docs.google.com/presentation/d/167i3QJ63EFYfiDpMIYtuQt2yS_x2Vm7J0_nTt2KnvtY/edit?usp=sharing
    tldr: Encourage weight sparsity in neural networks by introducing multiplicative binary gating variables along with each weight, and regularizing gates to be sparse.
    notes: 200+ citations &centerdot; <font color="crimson">Oral presentation</font> at Embedded Vision Workshop  
    type: short

  - authors:
      - S. Srinivas
    lastauthor: R.V. Babu
    year: 2016
    journal: Tech Report
    title: Generalized Dropout
    url: https://arxiv.org/abs/1611.06791
    tldr: A generalized version of dropout where dropout probabilities are automatically tuned during training. This is done by introducing multiplicative <i> bernoulli </i> gating variables to each neuron within a neural network, and modelling the bernoulli probability by penalizing from a beta distribution.
    type: short

  - authors:
      - S. Srinivas
    lastauthor: R.V. Babu
    year: 2016
    journal: BMVC
    title: Learning Neural Network Architectures using Backpropagation
    url: http://www.bmva.org/bmvc/2016/papers/paper104/index.html
    tldr: Automatically prune unimportant neurons during neural network training, by introducing multiplicative binary gating variables with each neuron, and encouraging the gate variables to be as sparse as possible via regularization.
    poster: https://drive.google.com/file/d/0BwC3N1sQOq3dRG1fRzlIcjc2c1k/view?usp=sharing&resourcekey=0-bb9N3jtW-co7yk3tvMYdZw
    type: long

  - authors:
      - L. Boominathan
      - S. Srinivas
    lastauthor: R.V. Babu
    year: 2016
    journal: ICVGIP
    title: Compensating for Large In-plane Rotations in Natural Images
    url: https://arxiv.org/abs/1611.05744
    tldr: Correct for large in-plane rotation in images by (1) detecting the presence of rotation using a CNN, and (2) correcting it iteratively using Bayesian optimization.
    poster: https://drive.google.com/file/d/1fVwYO6f9X7IjajO6Y101LiPuOa4LLfbi/view?usp=sharing
    type: short

  - authors:
      - S. Srinivas
      - R. Sarvadevabhatla
      - K.R. Mopuri
      - N. Prabhu
      - S.S. Kruthiventi
    lastauthor: R.V. Babu
    year: 2015
    journal: Frontiers in Robotics and AI
    title: A Taxonomy of Deep Convolutional Neural Nets for Computer Vision
    url: http://journal.frontiersin.org/article/10.3389/frobt.2015.00036/full
    notes:  
      300+ citations &centerdot; <font color="crimson">Top 25%</font> of all research outputs scored on <a href="https://frontiers.altmetric.com/details/4978361"> Altmetric </a> 
    tldr: A recipe-style survey of pre-2015 deep neural networks as applied to computer vision.
    type: long

  - authors:
      - S. Srinivas
    lastauthor: R.V. Babu
    year: 2015
    journal: BMVC
    title: Data-free Parameter Pruning for Deep Neural Networks
    url: http://arxiv.org/abs/1507.06149
    tldr: Prune neurons in neural networks by (1) identifying duplicate neuron pairs, (2) removing one and performing a `surgery` step to compensate for removal.
    poster: https://drive.google.com/file/d/0BwC3N1sQOq3dNmZ2cmFEZlZ5N1E/view?usp=sharing&resourcekey=0-tBl3eVM83YSB9lUlJUUu_A
    type: long
    notes: 600+ citations

  - authors:
      - S. Srinivas
      - A. Adiga
    lastauthor: C.S. Seelamantula
    year: 2014
    journal: ICIP
    title: Controlled blurring for improving image reconstruction quality in flutter-shutter acquisition
    url: http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7026178&tag=1
    tldr: Deliberately shaking a camera's sensor in a 2D plane during acquisition results in a well-defined blur kernel that can be used to deblur even in the presence of external camera shake.
    type: short