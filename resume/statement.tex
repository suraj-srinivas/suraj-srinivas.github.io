\documentclass{article}
\usepackage{geometry}
\usepackage{xcolor}

\geometry{
    a4paper,
    left=20mm,
    top=20mm,
    right=20mm,
    bottom=20mm
    }

\usepackage[round]{natbib}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue]{hyperref}
\renewcommand{\cite}{\citep}
%\usepackage{cmbright}
\usepackage{lmodern}
\renewcommand{\familydefault}{\sfdefault}
\usepackage[OT1]{fontenc}

%\setlength\parindent{0pt}

\title{Research Statement}
\author{Suraj Srinivas}

\date{}
\begin{document}
\definecolor{floralwhite}{RGB}{255,255,250}
\pagecolor{floralwhite}
\maketitle

\noindent Modern progress in machine learning is driven by large neural network architectures capable of handling vast amounts of data, resulting in remarkable capabilities across a diverse range of tasks. Yet, the immensity and complexity of these models present another challenge: understanding, iterating, and improving over them becomes difficult. Unlike mature engineering disciplines that advance through first-principles thinking, large-scale deep learning remains predominantly experimentally driven, guided by ad-hoc heuristics for critical aspects such as model architecture choice and the design of learning methods. As a result, well-performing deep learning systems are more akin to discoveries rather than deliberate designs, leading to an inadequate understanding of the fundamental principles underlying their remarkable generalization behavior.

\vspace*{0.3cm}
\noindent Building a principled view of deep learning models is vital for driving future progress for several reasons. Firstly, this can make the process of model building more \textbf{computationally efficient} and streamlined, reducing the need for time-consuming and costly hyperparameter searches. Secondly, a principled understanding of model behavior is instrumental for enabling \textbf{trustworthy machine learning}, which can help us to answer questions about how models arrive at their outputs in a human-interpretable manner. Finally, a principled view of machine intelligence can reveal the principles behind natural intelligence, such as animal or human intelligence.

\vspace*{0.3cm}
\noindent To this end, my research agenda is centered on improving our \textbf{mathematical and scientific understanding} of deep neural network models and the associated learning algorithms. I aim to make progress towards this high-level goal by \textbf{identifying concrete applications} that can benefit from an improved scientific understanding, such as computationally efficient and trustworthy machine learning, and \textbf{developing principled algorithms} for these use cases. A common theme in my research involves \textbf{identifying connections} between different areas of machine learning, such that advances made in one field can help progress in others. Through my research, I aspire to bridge the gap between the highly advanced engineering achievements in machine learning and the nascent state of its scientific understanding, to pave the way for more robust, efficient, and trustworthy machine learning systems.
 

\section*{Prior Work}

In the section below, I discuss my previous work in the areas of computationally efficient deep learning and trustworthy machine learning, including interpretability and model robustness. 

\subsection*{Computationally Efficient Deep Learning}
\paragraph{Model Pruning with Limited Data} Standard neural networks have highly redundant weights, making it possible to prune a large number of weight or neurons without sacrificing predictive accuracy. As a beginning masters student, I started gaining intuition for deep learning by visualizing neural activations in neural networks, when I observed many instances of nearly duplicate features. Inspired by this observation, we devised the first pruning method that removed duplicate neurons in standard pre-trained vision models \cite{srinivas2015data}. Critical to this approach was a `surgery' step that updated model weights to account for the neuron removal, without using training data. More recently, we generalized this approach \cite{halabi2022dataefficient} by \underline{casting pruning as submodular optimization}, leading to more efficient pruning algorithms under limited data settings.

\paragraph{Incorporating Sparsity in Neural Networks} When sufficient data is available for pruning, one can combine pruning with model training in several ways. One method we proposed is to relax the underlying discrete pruning optimization problem into a continuous one such that it is conductive to gradient-based learning, which we found to generalize to both neuron pruning \cite{srinivas2016learning} and weight pruning \cite{srinivas2017training} leading to sparse neural network models. Another method to achieve this is to alternate discrete pruning and model training, an algorithm called "iterative magnitude pruning". While this algorithm was initially proposed as a heuristic, we \underline{view pruning through the lens of compressed sensing} and related it to projected gradient descent (PGD), which yields optimal sparse solutions in some simple settings. Inspired by this connection, we improved iterative magnitude pruning by proposing cyclical pruning \cite{srinivas2022cyclical}, an algorithm that better approximates the optimal PGD solution and obtains improved compression ratios as a result.

\subsection*{Trustworthy Machine Learning}
Trustworthy machine learning is a rather broad area concerned with increasing model trust for end users and stakeholders. Within this, my work so far has been in the areas of post hoc interpretability and robust machine learning. 

\paragraph*{Understanding Feature Attribution Methods} Feature attribution methods are common in post hoc interpretability, which aims to find important input features that drive model behavior for a given input instance. However, the plethora of methods that exist in literature are ad-hoc heuristics, making it difficult to identify unifying formal goals for this field. To this end, we proposed the local function approximation framework \cite{han2022which} as a means of \underline{unification of feature methods}. This enables us to view several feature attribution methods, including several ones proposed as heuristics, as instances of a single underlying framework. This also helped us identify a no-free lunch theorem showing that no feature attribution method can fully achieve the goal of local interpretability absent additional specifications.

\vspace*{0.2cm}
\noindent Feature attribution methods are also often motivated via specific secondary properties. We showed \cite{srinivas2019full} that two such properties -- sensitivity and completeness -- were impossible to satisfy simultaneously, highlighting a \underline{fundamental limitation of feature attribution methods}. To alleviate this, we proposed the full-gradient representation which extracted feature importances at each layer of deep neural networks, and provably satisfied both the above properties.

\paragraph{Explaining Input-Gradient Attribution via Generative Modelling} An important yet unexplored question in interpretability is the question of why standard models are interpretable wrt their feature attributions in the first place when such interpretability is formally independent of generalization \cite{srinivas2021rethinking}. We explained this by invoking the latent energy-based generative properties $p(x \mid y)$ of softmax-based discriminate models $p(y \mid x)$ and showed that the training of this latent generative model specifically using score-matching \cite{hyvarinen2005estimation}, dictates gradient interpretability. Overall, this work shows that commonly used input-gradient methods do not capture information related to the discriminative model $p(y \mid x)$, but a latent generative model $p(x \mid y)$, and we must rethink its usage in practice. 

\paragraph*{Robustness by Regulating Local Geometry} While standard robust models often sacrifice predictive accuracy, this is not the case for the task of distillation, where we showed that ideas from robustness can make distillation more sample efficient \cite{srinivas2018knowledge}. Specifically we showed that Jacobian information at data points helps capture the local neighborhood around that data point, and thus provides more information to the student model to learn from the teacher model. 
We also showed applications in transfer learning using the same technique, which had similar benefits. Our work on explaining the structure of local geometry of deep models \cite{srinivas2021rethinking} also showed that robust models have similar generative modelling properties to model trained explicitly with generative regularizers. To explore this further, we investigated the inductive biases of robust models, and found that robust models tend often to have smaller curvature (in terms of Hessian norms) than non-robust ones \cite{srinivas2022efficient}, and We used this observation to propose new ways to training robust models based explicitly penalizing an efficient upper bound on model curvature.

\section*{Future Work}\label{sec2}
In the future, I would like to focus my research toward novel model architectures that improve data and compute efficiency of models. In addition, large models require improved interpretability methods, as current paradiagms have been shown to be conceptually ineffective \cite{lipton2018mythos} to address questions involving spurious correlations and precisely characterizing model behaviour. Toward this end, I propose three broad directions for future work: mechanistic interpretability and feature disentanglement, understanding memorization and data attribution in large models, and characterizing implicit biases of model architectures.    

\paragraph{Mechanistic Interpretability and Feature Disentanglement}

\paragraph{Understanding Memorization and Data Attribution in Generative Models}

\paragraph{Computational Efficiency via Adaptive Computation}

\vspace*{0.5cm}
\noindent Overall, these new lines of research can lead to a new generation of neural network models that are simultaneously more compute efficient and interpretable, enabling us to further scale large models while still being able to provide insight about their behaviour.

\bibliographystyle{apalike}
\bibliography{references}

\end{document}