---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: profile
title: about
---

I am a research scientist at Bosch Research (Sunnyvale, USA), where I work on computer vision problems for autonomous driving. My research involves developing interpretability tools that enable model understanding, debugging, and model editing.  

<details markdown="1">
<summary>Some representative methods (click to expand):</summary>
- [splice](https://arxiv.org/abs/2402.10376): A dictionary learning-like method to interpret CLIP models
- [discriminative feature attributions](https://arxiv.org/abs/2307.15007): A method to build discriminative models such that their saliency maps are faithful by design
- [fullgrad saliency](https://papers.nips.cc/paper/2019/hash/80537a945c7aaa788ccfcdf1b99b5d8f-Abstract.html): Layer-wise saliency maps for ReLU neural nets with cool mathematical properties (aka completeness)
</details>

<br>
I am also interested in the "science" of deep learning, i.e., systematic investigations of deep learning phenomena. For example, studying [forgetting dynamics in LLM training](https://arxiv.org/abs/2410.03249), or explaining observed links between [robustness and gradient interpretability](https://arxiv.org/abs/2305.19101). For more information, please see my <a href="/research_themes.html">research themes</a> and <a href="/publications.html">publications</a>.

I was previously a postdoctoral research fellow with [Hima Lakkaraju](https://himalakkaraju.github.io/) at Harvard University. I completed my PhD with [Fran√ßois Fleuret](https://fleuret.org/francois/), at [Idiap Research Institute](http://www.idiap.ch/en) & [EPFL](http://epfl.ch/), Switzerland.

<details markdown="1"> 
<summary>I am/was an organizer on (click to expand):</summary>
- the [theory of interpretable AI](https://tverven.github.io/tiai-seminar/) online seminar series
- [xai in action: past, present and future](https://xai-in-action.github.io/) workshop at NeurIPS 2023
- [interpretable ai: past, present and future](https://interpretable-ai-workshop.github.io/) workshop at NeurIPS 2024
- [interpretable ml](https://interpretable-ml-class.github.io/) course at Harvard, spring 2023
</details>

<br>
**Note**: If you are looking for mentorship / research collaborations on interpretability, feel free to reach out! 








