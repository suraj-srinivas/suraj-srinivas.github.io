---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: profile
title: about
---

I am a research scientist at Bosch Research (Sunnyvale, USA), where I work on computer vision problems for autonomous driving. My research interests are model interpretability and the "science" of deep learning, i.e., systematic investigations of deep learning phenomena. 

I started my research career with a masters at the [Indian Institute of Science](http://iisc.ac.in), Bangalore with [Prof. Venkatesh Babu](https://cds.iisc.ac.in/faculty/venky/). I completed my PhD with [Fran√ßois Fleuret](https://fleuret.org/francois/), at [Idiap Research Institute](http://www.idiap.ch/en) & [EPFL](http://epfl.ch/), Switzerland. Enamoured by model interpretability, I pursued a postdoctoral research fellowship with [Hima Lakkaraju](https://himalakkaraju.github.io/) at Harvard University.

Some representative papers include:
- [`splice`](https://arxiv.org/abs/2402.10376): shows that CLIP representations are approximate sparse linear combinations of concept vectors, using this to develop an interp tool that extracts concepts encoded by representations 
- [`explaining perceptually aligned gradients`](https://arxiv.org/abs/2305.19101): an experimental theory explaining why noise-robust models tend to produce improved (perceptually aligned) gradient saliency maps
- [`fullgrad saliency`](https://papers.nips.cc/paper/2019/hash/80537a945c7aaa788ccfcdf1b99b5d8f-Abstract.html): shows that ReLU neural net outputs can be exactly decomposed into layer-wise gradient terms, using this to develop a saliency tool that aggregates layer-wise gradients
- [`forgetting data contamination in LLMs`](https://arxiv.org/abs/2410.03249): an experimental study showing that data contamination in LLMs may be irrelevant in certain practical training scenarios

For more information, please see my <a href="/research_themes.html">research themes</a> and <a href="/publications.html">publications</a>.

**Note**: If you'd like to chat about the science or interpretability of neural nets, feel free to reach out!  



