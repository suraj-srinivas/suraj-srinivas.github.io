---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: profile
title: about
---


I am currently a postdoctoral research fellow in computer science at Harvard University where I work with [Prof. Hima Lakkaraju](https://himalakkaraju.github.io/). I completed my PhD at [Idiap Research Institute](http://www.idiap.ch/en) & [EPFL](http://epfl.ch/), Switzerland, where I was advised by [Prof. Fran√ßois Fleuret](https://www.idiap.ch/~fleuret/). 

[//]: # (Before this, I completed my Masters (by Research) at the [Indian Institute of Science, Bangalore](http://www.iisc.ac.in/) advised by [Prof. R. Venkatesh Babu](http://cds.iisc.ac.in/faculty/venky/). During my PhD, I interned at Qualcomm AI Research in Amsterdam, where I worked with [Tijmen Blankevoort](https://www.linkedin.com/in/tijmen-blankevoort-a5633a24/).)

I am interested in mathematically and scientifically understanding deep models and their associated learning algorithms, usually through the lens of **interpretability**, **robustness** and **computational efficiency** of models. 

- In **interpretability**, my work has [identified flaws](https://openreview.net/forum?id=dYeAHXnpWJ4) with feature attribution methods (aka "heatmap" explanations) and proposed [mathematical frameworks](https://arxiv.org/abs/2206.01254) to better understand them. I believe that interpretability cannot be an afterthought, and must be incorporated during model design. In line with this philosophy, our [recent](https://arxiv.org/abs/2307.15007) [works](https://arxiv.org/abs/2305.19101) connect interpretability with model robustness. I am also interested in [interpreting representations](https://arxiv.org/abs/2402.10376) of large-scale multi-modal models.

- In **robustness**, I am motivated by building models that are ["as simple as they need to be"](https://arxiv.org/abs/2206.07144) in a functional sense, corresponding to models that robust in the [average-case](https://arxiv.org/abs/2307.13885) rather than the "adversarial" worst-case. I am also interested in [certified robustness](https://arxiv.org/abs/2309.02705) for large language models.

- In **model efficiency**, I am motivated by building neural nets that are "as small as possible", usually by [pruning weights](https://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Srinivas_Cyclical_Pruning_for_Sparse_Neural_Networks_CVPRW_2022_paper.html) or neurons. I also developed a (highly-cited) [method](https://arxiv.org/abs/1507.06149) to prune neurons using minimal training data.

For more details, please see my latest <a href="/publications.html">publications</a>.

<p style="border-width:1px; border-style:solid; background-color:mintcream; padding: 1em; border-radius: 10px">
<b>I'm on the job market</b>. Contact me regarding interesting opportunities!
</p>



