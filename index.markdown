---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: profile
title: about
---

<p style="border-width:1px; padding:2%; border-style:solid; border-radius: 8px">
<b>I'm on the job market</b>. Contact me regarding interesting opportunities!
</p>

I am a postdoctoral research fellow in computer science at Harvard University where I work with [Prof. Hima Lakkaraju](https://himalakkaraju.github.io/). I completed my PhD from [Idiap Research Institute](http://www.idiap.ch/en) & [EPFL](http://epfl.ch/), Switzerland, advised by [Prof. Fran√ßois Fleuret](https://www.idiap.ch/~fleuret/). 

I previously completed my Masters (by Research) at the [Indian Institute of Science, Bangalore](http://www.iisc.ac.in/) advised by [Prof. R. Venkatesh Babu](http://cds.iisc.ac.in/faculty/venky/). During my PhD, I interned at Qualcomm AI Research in Amsterdam, where I worked with [Tijmen Blankevoort](https://www.linkedin.com/in/tijmen-blankevoort-a5633a24/).

I am interested in mathematically and scientifically understanding deep learning, usually through the lens of **interpretability**, **robustness** and **computational efficiency** of models. I typically study these in the context of **computer vision** and more recently, **natural language processing**. 

- In **interpretability**, my work has [identified flaws](https://openreview.net/forum?id=dYeAHXnpWJ4) with feature attribution methods (aka "heatmap" explanations), proposed unifying [mathematical frameworks](https://arxiv.org/abs/2206.01254) to better understand them, and [explained](https://arxiv.org/abs/2305.19101) why they appear human-aligned for robust models. I have also worked on [interpreting representations](https://arxiv.org/abs/2402.10376) of CLIP models via sparse concept decompositions.

- In **robustness**, I am motivated by Occam's razor, building [low curvature](https://arxiv.org/abs/2206.07144) models that are "as simple as possible" in a functional sense, and models that are robust in the [average-case](https://arxiv.org/abs/2307.13885) rather than the "adversarial" worst-case. I have also worked on [certified robustness](https://arxiv.org/abs/2309.02705) to large language model jailbreaks.

- In **model efficiency**, I have worked on building neural nets with sparse weights, usually by [pruning](https://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Srinivas_Cyclical_Pruning_for_Sparse_Neural_Networks_CVPRW_2022_paper.html) [redundant weights](https://arxiv.org/abs/1611.06694) in dense models. I have also developed a [method](https://arxiv.org/abs/1507.06149) to prune neurons using minimal training data.

For more details, please see my latest <a href="/publications.html">publications</a>. 

I was also an organizer for the [XAI in Action](https://xai-in-action.github.io/) workshop at NeurIPS 2023, and was involved in teaching an [interpretable ML](https://interpretable-ml-class.github.io/) course at Harvard during spring 2023.





