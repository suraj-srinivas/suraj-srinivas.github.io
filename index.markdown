---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: profile
title: about
---


I am currently a postdoctoral research fellow in computer science at Harvard University where I work with [Prof. Hima Lakkaraju](https://himalakkaraju.github.io/). I completed my PhD at [Idiap Research Institute](http://www.idiap.ch/en) & [EPFL](http://epfl.ch/), Switzerland, where I was advised by [Prof. Fran√ßois Fleuret](https://www.idiap.ch/~fleuret/). 

[//]: # (Before this, I completed my Masters (by Research) at the [Indian Institute of Science, Bangalore](http://www.iisc.ac.in/) advised by [Prof. R. Venkatesh Babu](http://cds.iisc.ac.in/faculty/venky/). During my PhD, I interned at Qualcomm AI Research in Amsterdam, where I worked with [Tijmen Blankevoort](https://www.linkedin.com/in/tijmen-blankevoort-a5633a24/).)

I am interested in mathematically and scientifically understanding deep models and their associated learning algorithms, usually through the lens of **interpretability**, **robustness** and **computational efficiency** of models. 

- In **interpretability**, my work has [identified flaws](https://openreview.net/forum?id=dYeAHXnpWJ4) with feature attribution methods (aka "heatmap" explanations), proposed unifying [mathematical frameworks](https://arxiv.org/abs/2206.01254) to better understand them, and [explained](https://arxiv.org/abs/2305.19101) why they appear human-aligned for robust models. I have also worked on [interpreting representations](https://arxiv.org/abs/2402.10376) of large-scale multi-modal models.

- In **robustness**, I am motivated by building models that are ["as simple as possible"](https://arxiv.org/abs/2206.07144) in a functional sense, and models that are robust in the [average-case](https://arxiv.org/abs/2307.13885) rather than the "adversarial" worst-case. I have also worked on [certified robustness](https://arxiv.org/abs/2309.02705) for large language models.

- In **model efficiency**, I have worked on building neural nets with sparse weights, usually by [pruning](https://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Srinivas_Cyclical_Pruning_for_Sparse_Neural_Networks_CVPRW_2022_paper.html) [redundant weights](https://arxiv.org/abs/1611.06694) in dense models. I have also developed a (highly-cited) [method](https://arxiv.org/abs/1507.06149) to prune neurons using minimal training data.

For more details, please see my latest <a href="/publications.html">publications</a>.

<p style="border-width:1px; border-style:none; border-radius: 5%">
<b>I'm on the job market</b>. Contact me regarding interesting opportunities!
</p>



